\section{Evaluation} \label{sec:evaluation}

\subsection{Experimental Setup}

We implement a prototype of \SYSTEM{} on a Hardkernel Odroid XU3 ARM big.LITTLE
system (Samsung Exynos5422 A15 and A7 quad core CPUs, 2Gbyte LPDDR3 RAM, eMMC5.0
HS400 backing store) running Ubuntu Trusty 14.04 LTS, kernel version 3.10.58. 
% We use the energymon portable interface~\cite{energymon} to measure
% instantaneous energy and power consumption by polling the odroid's
% internal INA-231 power sensors.

\subsection{Experimental Results}

In this section we seek to answer four questions:
\begin{enumerate}
\item What is \SYSTEM{}'s overhead when compared to dm-crypt AES-XTS?
\item How does the \SYSTEM{} under an LFS (\ie F2FS) configuration compare to
the popular dm-crypt under Ext4 configuration?
\item Where does \SYSTEM{} derive its performance gains? Implementation? Choice
of cipher?
\item How does \SYSTEM{} perform when the backing store is nearly full?
\end{enumerate}

To evaluate the performance of \SYSTEM{}, we measure the latency
(seconds/milliseconds per operation) of both sequential and random read and
write I/O operations across four different standard Linux filesystems: NILFS2,
F2FS, Ext4 in ordered journaling mode, and Ext4 in full journaling mode. The I/O
operations are performed using file sizes between 4KB and 40MB. These files
were populated with random data; this data remained constant throughout the
evaluation. The experiments were performed using a standard Linux ramdisk
(tmpfs) as the ultimate backing store.

Ext4 ordered journaling mode (\texttt{data=ordered}) is the default mode of
Ext4, where metadata is committed to the filesystem's journal while
the actual data is written through to the main filesystem. In the event of a
crash, the filesystem can use the journal to avoid damage and recover to a
consistent state. Full journaling mode (\texttt{data=journal}) journals both
metadata and the filesystem's actual data---essentially a double write-back for
each write operation. In the event of a crash, the journal can be used to replay
entire I/O events so that both the filesystem and its data can be recovered. We
include both modes of Ext4 to further explore the impact of frequent overwrites
against \SYSTEM{}.

The experiment consists of reading and writing each file in its entirety 30
times sequentially, and then reading and writing random portions of each file 30
times. In both cases, the same amount of data was read and written per file. The
median latency was taken per result set. We chose 30 read/write operations (10
read/write operations repeated three times each) to handle potential variation.
The Linux page cache was dropped before every read operation, each file was
opened in synchronous I/O mode via \texttt{O\_SYNC}, and we relied on non-
buffered \texttt{read()}/\texttt{write()} system calls. A high-level I/O size of
128KB was used for all read and write calls that hit the filesystems; however,
the I/O requests being made at the block device layer varied between 4KB and
128KB depending on the filesystem under test.

The experiment was repeated on each filesystem in three different
configurations:

\begin{enumerate}
\item \textit{unencrypted}: Filesystem mounted atop a BUSE virtual block
  device set up to immediately pass through any incoming I/O requests straight
  to the backing store. We use this as the baseline measurement of the
  filesystem's performance without any encryption.
\item \textit{\SYSTEM{}}: Filesystem mounted atop a BUSE virtual block
  device provided by our \SYSTEM{} implementation to perform full-disk
  encryption.
\item \textit{dm-crypt}: Filesystem mounted atop a Device Mapper
 ~\cite{LinuxDeviceMapper} higher-level virtual block device provided by
  dm-crypt to perform full-disk encryption, which itself is mounted atop a
  BUSE virtual block device with pass through behavior identical to the device
  used in the baseline configuration. dm-crypt was configured to use AES-XTS as
  its full-disk encryption algorithm. All other parameters were left at their
  default values.
\end{enumerate}

\figref{microbench-f2fs} compares \SYSTEM{} to dm-crypt under the F2FS
filesystem.  The gamut of result sets over different filesystems can
be seen in \figref{microbench-gamut}.  \figref{microbench-ext4}
compares Ext4 with dm-crypt to F2FS with \SYSTEM{}.  

\begin{figure}[ht]
    \textbf{\SYSTEM{} vs dm-crypt AES-XTS: F2FS Test}\par\medskip
    \begin{subfigure}{\linewidth}
        \centering
        {\input{img/microbench-f2fs-sequential.tex}}
        \caption{Sequential I/O expanded F2FS result set.}
        \label{fig:microbench-f2fs-sequential}
    \end{subfigure}\\[1ex]
    \begin{subfigure}{\linewidth}
        \centering
        {\input{img/microbench-f2fs-random.tex}}
        \caption{Random I/O expanded F2FS result set.}
        \label{fig:microbench-f2fs-random}
    \end{subfigure}
    \caption{Test of the F2FS LFS mounted atop both dm-crypt and
      \SYSTEM{}; median latency of different sized whole file read and
      write operations normalized to unencrypted access. By harmonic
      mean, \SYSTEM{} is 1.6$\times$ faster than dm-crypt for reads
      and 1.2$\times$ faster for writes.}
    \label{fig:microbench-f2fs}
\end{figure}

\subsection{\SYSTEM{} Read Performance}

\figref{microbench-f2fs} shows the performance of \SYSTEM{} in comparison to
dm-crypt, both mounted with the F2FS filesystem. We see \SYSTEM{} improves on
the performance of dm-crypt's AES-XTS implementation across sequential and
random read operations on all file sizes. Specifically, $2.07\times$ for
sequential 40MB, $2.08\times$ for sequential 5MB, $1.85\times$ for sequential
512KB, and $1.03\times$ for sequential 4KB.

\figref{microbench-gamut} provides an expanded performance profile for
\SYSTEM{}, testing a gamut of filesystems broken down by workload file size. For
sequential reads across all filesystems and file sizes, \SYSTEM{} outperforms
dm-crypt. This is true even on the non-LFS Ext4 filesystems. Specifically, we
see read performance improvements over dm-crypt AES-XTS for 40MB sequential
reads of $2.02\times$ for NILFS, $2.07\times$ for F2FS, $2.09\times$ for Ext4 in
ordered journaling mode, and $2.06\times$ for Ext4 in full journaling mode. For
smaller file sizes, the performance improvement is less pronounced.
Specifically, for 4KB reads we see $1.28\times$ for NILFS, $1.03\times$ for
F2FS, $1.07\times$ for Ext4 in ordered journaling mode, and $1.04\times$ for
Ext4 in full journaling mode.

When it comes to random reads, we see virtually identical results save for 4KB
reads, where dm-crypt proved very slightly more performant under the NILFS
LFS at $1.12\times$. This behavior was not observed under the more modern F2FS
LFS.

\PUNT{These results demonstrate the claim from the introduction: under Log- structured
File Systems, \SYSTEM{} improves read performance over more traditional AES-XTS
based full disk encryption schemes by upwards of $2\times$ while maintaining a
confidentiality guarantee and additionally providing a data integrity guarantee.
Further, these results suggest that implementing a stream-cipher based full-disk
encryption scheme with added integrity protection is not so expensive as to make
it untenable. Indeed, we can achieve significant performance wins by leveraging
the speed of fast stream ciphers, especially on systems with a heavy read bias,
\ie web servers, media hosts, and some mobile devices.}


\begin{figure*}[t]
    \textbf{\SYSTEM{} Four Filesystems Test}\par\medskip
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        {\input{img/microbench-gamut-sequential-r.tex}}
        \caption{Sequential reads.}
        \label{fig:microbench-gamut-sequential-r}
    \end{subfigure}\hspace*{0.5em}%
    \begin{subfigure}{0.5\linewidth}
        \centering
        {\input{img/microbench-gamut-sequential-w.tex}}
        \caption{Sequential writes.}
        \label{fig:microbench-gamut-sequential-w}
    \end{subfigure}\\[1ex]
    \hspace*{-0.9em}%
    \begin{subfigure}{0.5\linewidth}
        \vspace{0.5em}
        \centering
        {\input{img/microbench-gamut-random-r.tex}}
        \caption{Random reads.}
        \label{fig:microbench-gamut-random-r}
    \end{subfigure}%
    \begin{subfigure}{0.5\linewidth}
        \centering
        {\input{img/microbench-gamut-random-w.tex}}
        \caption{Random writes.}
        \label{fig:microbench-gamut-random-w}
    \end{subfigure}
    \caption{Comparison of four filesystems running on top of
      \SYSTEM{} performance is normalized to the same file system
      running on dm-crypt.  Points below unity signify \SYSTEM{}
      outperforming dm-crypt. Points above the line signify dm-crypt
      outperforming \SYSTEM{}.}
    \label{fig:microbench-gamut}
\end{figure*}


\subsection{\SYSTEM{} Write Performance}

\figref{microbench-f2fs} shows the performance of \SYSTEM{} in comparison to
dm-crypt under the modern F2FS LFS broken down by workload file size. Similar to
read performance under the F2FS, we see \SYSTEM{} improves on the performance of
dm-crypt's AES-XTS implementation across sequential and random write operations
on all file sizes. Hence, \SYSTEM{} under F2FS is holistically faster than
dm-crypt under F2FS. Specifically, $1.33\times$ for sequential 40MB, $1.21\times$
for sequential 5MB, $1.15\times$ for sequential 512KB, and $1.19\times$ for
sequential 4KB.

\figref{microbench-gamut} provides an expanded performance profile for
\SYSTEM{}, testing a gamut of filesystems broken down by workload file size.
Unlike read performance, write performance under certain filesystems is more of
a mixed bag. For 40MB sequential writes, \SYSTEM{} outperforms dm-crypt's AES-
XTS implementation by $1.33\times$ for F2FS and $1.18\times$ for NILFS. When it
comes to Ext4, \SYSTEM{}'s write performance drops precipitously with a
$3.6\times$ \textit{slowdown} for both ordered journaling and full journaling
modes. For non-LFS 4KB writes, the performance degradation is even more
pronounced with a $8.09\times$ slowdown for ordered journaling and $14.5\times$
slowdown for full journaling.

This slowdown occurs in Ext4 because, while writes in \SYSTEM{} from non-LFS
filesystems have a metadata overhead that is comparable to that of forward
writes in an LFS filesystem, Ext4 is not an append-only or append-mostly
filesystem. This means that, at any time, Ext4 will initiate one or more
overwrites anywhere on the disk (see \tblref{overwrites}). As described in
\secref{design}, overwrites once detected trigger the rekeying process, which is
a relatively expensive operation. Multiple overwrites compound this expense
further. This makes Ext4 and other filesystems that do not exhibit at least
append-mostly behavior unsuitable for use with \SYSTEM{}. We include it in our
result set regardless to illustrate the drastic performance impact of frequent
overwrites on \SYSTEM{}.

For both sequential and random 4KB writes among the LFSes, the performance
improvement over dm-crypt's AES-XTS implementation for LFSes deflates. For
the more modern F2FS atop \SYSTEM{}, there is a $1.19\times$ improvement. For
the older NILFS filesystem atop \SYSTEM{}, there is a $2.38\times$ slowdown.
This is where we begin to see the overhead associated with tracking writes and
detecting overwrites potentially becoming problematic, though the overhead is
negligible depending on choice of LFS and workload characteristics.

These results show that \SYSTEM{} is sensitive to the behavior of the LFS that
is mounted atop it, and that any practical use of \SYSTEM{} would require an
extra profiling step to determine which LFS works best with a specific workload.
With the correct selection of LFS, such as F2FS for workloads dominated by small
write operations, potential slowdowns when compared to mounting that same
filesystem over dm-crypt's AES-XTS can be effectively mitigated.


\subsection{On Replacing dm-crypt and Ext4}

\begin{figure}[ht]
    \textbf{\SYSTEM{} F2FS vs dm-crypt AES-XTS Ext4-OJ}\par\medskip
    \begin{subfigure}{\linewidth}
        \centering
        {\input{img/microbench-ext4-sequential.tex}}
        \caption{Sequential I/O F2FS vs Ext4 result set.}
        \label{fig:microbench-ext4-sequential}
    \end{subfigure}\\[1ex]
    \begin{subfigure}{\linewidth}
        \centering
        {\input{img/microbench-ext4-random.tex}}
        \caption{Random I/O F2FS vs Ext4 result set.}
        \label{fig:microbench-ext4-random}
    \end{subfigure}
    \caption{ Comparison of Ext4 on dm-crypt and F2FS on \SYSTEM{}.
      Results are normalized to unencrypted Ext4 performance.
      Unecrypted F2FS results are shown for reference.}
    \label{fig:microbench-ext4}
\end{figure}

\figref{microbench-ext4} describes the performance benefit of using \SYSTEM{}
with F2FS over the popular dm-crypt with Ext4 in ordered journaling mode
combination for both sequential and random read and write operations of various
sizes. Other than 4KB write operations, which is an instance where baseline F2FS
without \SYSTEM{} is simply slower than baseline Ext4 without dm-crypt,
\SYSTEM{} with F2FS outperforms dm-crypt's AES-XTS implementation with Ext4.

These results show that configurations taking advantage of the popular
combination of dm-crypt, AES-XTS, and Ext4 could see a significant improvement
in read performance without a degradation in write performance except in cases
where small ($\approx4KB$) writes dominate the workload.

Note, however, that several implicit assumptions exist in our design. For one,
we presume there is ample memory at hand to house the Merkle Tree and all other
data abstractions used by \SYSTEM{}. Efficient memory use was not a goal of our
implementation of \SYSTEM{}. In an implementation aiming to be production ready,
much more memory efficient data structures would be utilized.

It is also for this reason that populating the Merkle Tree necessitates a rather
lengthy mounting process. In our tests, a 1GB backing store on the odroid system
can take as long as 15 seconds to mount.

\subsection{Performance in \SYSTEM{}: ChaCha20 vs AES}

\figref{microbench-gamut} and \figref{microbench-f2fs} give strong evidence for
our general performance improvement over dm-crypt not being an artifact of
filesystem choice. Excluding Ext4 as a non-LFS filesystem under which to run
\SYSTEM{}, our tests show that \SYSTEM{} outperforms dm-crypt under an LFS
filesystem in the vast majority of outcomes.

We then test to see if our general performance improvement can be attributed
to the use of a stream cipher over a block cipher.
\figref{microbench-aes-vs-chacha} describes the relationship between ChaCha20,
the cipher of choice for our implementation of \SYSTEM{}, and the AES cipher.
dm-crypt implements AES in XTS mode to provide full-disk encryption
functionality. Swapping out ChaCha20 for AES-CTR resulted in slowdowns of up to
$1.6\times$ for reads and $1.23\times$ for writes across all configurations, as
described in \figref{microbench-aes-vs-chacha}.

Finally, we test to see if our general performance improvement can be attributed to
our implementation of \SYSTEM{} rather than our choice of stream cipher. We test
this by implementing AES in XTS mode on top of \SYSTEM{} using OpenSSL EVP.
\SYSTEM{} using OpenSSL AES-XTS experiences slowdowns of up to $1.33\times$ for
reads and $1.15\times$ for writes across all configurations compared to
\SYSTEM{} using ChaCha20. Interestingly, while significantly less performant,
this slowdown is not entirely egregious, and suggests that perhaps there are
parts of the dm-crypt code base that would benefit from further optimization.

\begin{figure}[ht]
    \textbf{ChaCha20 vs AES: \SYSTEM{} F2FS Sequential Test}\par\medskip
    \centering
    {\input{img/microbench-aes-vs-chacha.tex}}
    \caption{Comparison of AES in XTS and CTR modes versus ChaCha20 in
      \SYSTEM{}; median latency of different sized whole file
      sequential read and write operations normalized to ChaCha20
      (default cipher in \SYSTEM{}). Points below unity signify AES
      outperforming ChaCha20. Points above the line signify ChaCha20
      outperforming AES.}
    \label{fig:microbench-aes-vs-chacha}
\end{figure}

\subsection{Overhead with a Full Disk}

During I/O operations under an appropriate choice of LFS, we have shown that
full-disk encryption provided by \SYSTEM{} outperforms full-disk encryption
provided by dm-crypt. However, this is not necessarily the case when the backing
store becomes full and the LFS is forced to cope with an inability to write
forward as efficiently.

In the case of the F2FS LFS, upon approaching capacity and being unable to
perform garbage collection effectively, it resorts to writing blocks out to
where ever it can find free space in the backing store~\cite{F2FS}. It does this
instead of trying to maintain an append-only guarantee. This method of executing
writes is similar to how a typical non-LFS filesystem operates. When this
happens, the F2FS aggressively causes overwrites in \SYSTEM{}, which has a
drastic impact on performance.

\figref{microbench-f2fs-full} shows the impact of these (sequential) overwrites.
Read operation performance remains faster on a full \SYSTEM{} backing store
compared to dm-crypt. This is not the case with writes. Compared to \SYSTEM{}
under non-full conditions, 40MB sequential writes were slowed by up to
$3.76\times$ as \SYSTEM{} approached maximum capacity.

Depending on the chosen garbage collection strategy (see
\secref{implementation}), an LFS mounted atop a proper implementation of
\SYSTEM{} would be prevented from reaching maximum capacity.

\begin{figure}[ht]
    \textbf{Near-Full Disk F2FS Test}\par\medskip
    \centering
    {\input{img/microbench-f2fs-full.tex}}
    \caption{Comparison of F2FS baseline, atop dm-crypt, and atop
      \SYSTEM{}. All configurations are initialized with a near-full
      backing store; median latency of different sized whole file read
      and write operations normalized to dm-crypt. Points below the
      line are outperforming dm-crypt. Points above the line are
      underperforming compared to dm-crypt.}
    \label{fig:microbench-f2fs-full}
\end{figure}
